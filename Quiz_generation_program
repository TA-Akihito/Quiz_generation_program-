#ライブラリとAPIの準備
!pip install -U openai
!pip install google_api
!pip install google-api-python-client llama_index langchain
!pip install search_google
!pip install sentence-transformers
!pip install sentencepiece --upgrade
!pip install requests
!pip install beautifulsoup4


import openai
import re
import os

import search_google
from googleapiclient.discovery import build
import requests
from llama_index.readers import BeautifulSoupWebReader
from llama_index import LLMPredictor, ServiceContext
from bs4 import BeautifulSoup
from transformers import GPT2Tokenizer

import torch
from sentence_transformers import SentenceTransformer
from transformers import MLukeTokenizer, LukeModel
from sklearn.metrics.pairwise import cosine_similarity
import unicodedata

import datetime
from datetime import timedelta

import random
from langchain.llms import OpenAI
from langchain import LLMChain, PromptTemplate


# GPT3.5&4のAPIシークレットキー
SECRET_KEY  = ""

# API認証情報設定
os.environ["OPENAI_API_KEY"] = SECRET_KEY

# GPT3.5&4のAPIシークレットキー
openai.api_key = ""

#Google Search APIのシークレットキー
api_key = ""

#カスタム検索エンジンID
cx = ""


# Google Search APIを実行する関数
def search_google(query, api_key, cx, num_results=10):
    base_url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cx,
        "q": query,
        "num": num_results
    }
    response = requests.get(base_url, params=params)

    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Google API request failed with status code {response.status_code}")


# 類似度を計るモデル
class SentenceLukeJapanese:
    def __init__(self, model_name_or_path, device=None):
        self.tokenizer = MLukeTokenizer.from_pretrained(model_name_or_path)
        self.model = LukeModel.from_pretrained(model_name_or_path)
        self.model.eval()

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)
        self.model.to(device)

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def encode(self, sentences, batch_size=8):
        all_embeddings = []
        iterator = range(0, len(sentences), batch_size)
        for batch_idx in iterator:
            batch = sentences[batch_idx:batch_idx + batch_size]

            encoded_input = self.tokenizer.batch_encode_plus(batch, padding="longest",
                                           truncation=True, return_tensors="pt").to(self.device)
            model_output = self.model(**encoded_input)
            sentence_embeddings = self._mean_pooling(model_output, encoded_input["attention_mask"]).to('cpu')

            all_embeddings.extend(sentence_embeddings)

        return torch.stack(all_embeddings)

model_name = 'sonoisa/sentence-luke-japanese-base-lite'
model = SentenceLukeJapanese(model_name)


# urlからテキストを取得する/類似度を計る関数
def get_text_from_url(url):
    headers = {
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0",
      "Accept-Language": "ja,en;q=0.5",
      "Accept-Encoding": "gzip, deflate, br",
      "Connection": "keep-alive"
    }

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            text = ' '.join([p.get_text() for p in soup.find_all("p")])
            return text
        else:
            print(f"Request to {url} failed with status code {response.status_code}")
            return None
    except Exception as e:
        print(f"Error fetching data from {url}: {e}")
        return None

def calculate_relevance_score_st(query, documents):
    embeddings = model.encode([query] + documents)
    cosine_similarities = cosine_similarity(embeddings[0:1].detach(), embeddings[1:].detach()).flatten()
    return cosine_similarities


# GPT-3トークナイザー
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')


# 生成するクイズのトピックを入力
topic = "呪術廻戦"


# 15日経過したデータを削除
if os.path.exists("test.txt"):
    current_date = datetime.datetime.now()

    # ファイルを読み込み、各行を解析
    with open('test.txt') as f:
        content = f.readlines()

    new_content = []
    erase_data = []
    found = False

    # 書き込まれてから15日経過したデータを削除
    for line in content:

        # 日付形式がある行を抽出
        match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},', line, re.DOTALL)
        if match:

            # 日付とテキストを分離
            date_string, text = line.split(",", 1)
            line_date = datetime.datetime.strptime(date_string.strip(), "%Y-%m-%d %H:%M:%S")

            # 現在の日付と行の日付が15日以上なら、その行を削除するデータを格納するリストに追加
            if (current_date - line_date) >= timedelta(days = 15):
                topic_line = line.strip()
                topic_line_name =  topic_line.split(",",1)[1]
                content_str = ''.join(content)
                match = re.search(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}" + f',{re.escape(topic_line_name)}\n始め\n(.*)\n{re.escape(topic_line_name)}\n終わり', content_str, re.DOTALL)
                if match:
                    found = True
                    erase_data.append(match.group(0))
            else:
                new_content.append(line)
        else:
            new_content.append(line)

    if found:
        print("True")

        # 15日以上経過したデータを元データから削除
        for i in range(len(erase_data)):
            if i==0:
                processed_content = content_str.replace(erase_data[i],'')
            else:
                processed_content = processed_content.replace(erase_data[i],'')
        with open("test.txt",'w') as f:
            f.write(processed_content)
    else:
        print("False")

        # 元データをそのまま書き込む
        with open('test.txt', 'w') as f:
            for line in new_content:
                f.write(line)


# ファイルの中身を確認
if os.path.exists("test.txt"):
    with open('test.txt') as f:
        state_of_the_union = f.read()
        print(state_of_the_union)


# 記憶の活用（テキストファイル内のデータをクイズ作成に用いる）
if os.path.exists("test.txt"):
    with open('test.txt', 'r') as f:
        content = f.read()

        match = re.search(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}" + f',{re.escape(topic)}\n始め\n(.*)\n{re.escape(topic)}\n終わり', content, re.DOTALL)
        if match:
            all_data = match.group(0)

            # トークン数で行ごとに分割
            text_all = all_data
            text = text_all.replace('', '')
            lines = text.replace("。", "。\n").replace(".",".\n").split('\n')

            # 空行を削除
            lines = [line for line in lines if line.strip() != ""]

            # 行数をカウント
            line_count = len(lines)
            lines = "".join(lines)
            tokens_all = tokenizer.tokenize(text)

            if len(tokens_all) > 6800:
                line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                total = 0
                index_list = []
                for i, length in enumerate(line_lengths):
                    total += length
                    if total > 6800:
                        index_list.append(i)
                        total = line_lengths[i]

            else:
                index_list = []

            if len(tokens_all) > 6800:

                # 6800トークン以下になるようにデータを行ごとに分割
                for i in range(len(index_list)):
                    over_7000 = index_list[i]

                    if i == 0:
                      first_group_end = over_7000
                      first_group_lines = lines[0 : first_group_end]
                      first_group_text = ''.join(first_group_lines)
                      if i == 0 and len(index_list) == 1:

                          #最後の行を範囲に含める
                          end = line_count + 1
                          remain_group = lines[first_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 1:
                      second_group_end = over_7000
                      second_group_lines = lines[first_group_end : second_group_end]
                      second_group_text = ''.join(second_group_lines)
                      if i == 1 and len(index_list) == 2:
                          end = line_count + 1
                          remain_group = lines[second_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 2:
                      third_group_end  = over_7000
                      third_group_lines = lines[second_group_end : third_group_end]
                      third_group_text = ''.join(third_group_lines)
                      if i == 2 and len(index_list) == 3:
                          end = line_count + 1
                          remain_group = lines[third_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 3:
                      fourth_group_end = over_7000
                      fourth_group_lines = lines[third_group_end : fourth_group_end]
                      fourth_group_text = ''.join(fourth_group_lines)
                      if i == 3 and len(index_list) == 4:
                          end = line_count + 1
                          remain_group = lines[fourth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    print("--------------------------------------")

                    if i == 4:
                      fifth_group_end = over_7000
                      fifth_group_lines = lines[fourth_group_end : fifth_group_end]
                      fifth_group_text = ''.join(fifth_group_lines)
                      if i == 4 and len(index_list) == 5:
                          end = line_count + 1
                          remain_group = lines[fifth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 5:
                      sixth_group_end = over_7000
                      sixth_group_lines = lines[fifth_group_end : sixth_group_end]
                      sixth_group_text = ''.join(sixth_group_lines)
                      if i == 5 and len(index_list) == 6:
                          end = line_count + 1
                          remain_group = lines[sixth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 6:
                      seventh_group_end = over_7000
                      seventh_group_lines = lines[sixth_group_end : seventh_group_end]
                      seventh_group_text = ''.join(seventh_group_lines)
                      if i == 6 and len(index_list) == 7:
                          end = line_count + 1
                          remain_group = lines[seventh_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 7:
                      eighth_group_end = over_7000
                      eighth_group_lines = lines[seventh_group_end : eighth_group_end]
                      eighth_group_text = ''.join(eighth_group_lines)
                      if i == 7 and len(index_list) == 8:
                          end = line_count + 1
                          remain_group = lines[eighth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 8:
                      ninth_group_end = over_7000
                      ninth_group_lines = lines[eighth_group_end : ninth_group_end]
                      ninth_group_text = ''.join(ninth_group_lines)
                      if i == 8 and len(index_list) == 9:
                          end = line_count + 1
                          remain_group = lines[ninth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 9:
                      tenth_group_end = over_7000
                      tenth_group_lines = lines[ninth_group_end : tenth_group_end]
                      tenth_group_text = ''.join(tenth_group_lines)
                      if i == 9 and len(index_list) == 10:
                          end = line_count + 1
                          remain_group = lines[tenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 10:
                      eleventh_group_end = over_7000
                      eleventh_group_lines = lines[tenth_group_end : eleventh_group_end]
                      eleventh_group_text = ''.join(eleventh_group_lines)
                      if i == 10 and len(index_list) == 11:
                          end = line_count + 1
                          remain_group = lines[eleventh_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 11:
                      twelveth_group_end = over_7000
                      twelveth_group_lines = lines[eleventh_group_end : twelveth_group_end]
                      twelveth_group_text = ''.join(twelveth_group_lines)
                      if i == 11 and len(index_list) == 12:
                          end = line_count + 1
                          remain_group = lines[twelveth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 12:
                      thirteenth_group_end = over_7000
                      thirteenth_group_lines = lines[twelveth_group_end : thirteenth_group_end]
                      thirteenth_group_text = ''.join(thirteenth_group_lines)
                      if i == 12 and len(index_list) == 13:
                          end = line_count + 1
                          remain_group = lines[thirteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 13:
                      fourteenth_group_end = over_7000
                      fourteenth_group_lines = lines[thirteenth_group_end : fourteenth_group_end]
                      fourteenth_group_text = ''.join(fourteenth_group_lines)
                      if i == 13 and len(index_list) == 14:
                          end = line_count + 1
                          remain_group = lines[fourteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 14:
                      fifteenth_group_end = over_7000
                      fifteenth_group_lines = lines[fourteenth_group_end : fifteenth_group_end]
                      fifteenth_group_text = ''.join(fifteenth_group_lines)
                      if i == 14 and len(index_list) == 15:
                          end = line_count + 1
                          remain_group = lines[fifteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 15:
                      sixteenth_group_end = over_7000
                      sixteenth_group_lines = lines[fifteenth_group_end : sixteenth_group_end]
                      sixteenth_group_text = ''.join(sixteenth_group_lines)
                      if i == 15 and len(index_list) == 16:
                          end = line_count + 1
                          remain_group = lines[sixteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 16:
                      seventeenth_group_end = over_7000
                      seventeenth_group_lines = lines[sixteenth_group_end : seventeenth_group_end]
                      seventeenth_group_text = ''.join(seventeenth_group_lines)
                      if i == 16 and len(index_list) == 17:
                          end = line_count + 1
                          remain_group = lines[seventeenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 17:
                      eighteenth_group_end = over_7000
                      eighteenth_group_lines = lines[seventeenth_group_end : eighteenth_group_end]
                      eighteenth_group_text = ''.join(eighteenth_group_lines)
                      if i == 17 and len(index_list) == 18:
                          end = line_count + 1
                          remain_group = lines[eighteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 18:
                      nineteenth_group_end = over_7000
                      nineteenth_group_lines = lines[eighteenth_group_end : nineteenth_group_end]
                      nineteenth_group_text = ''.join(nineteenth_group_lines)
                      if i == 18 and len(index_list) == 19:
                          end = line_count + 1
                          remain_group = lines[nineteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 19:
                      twentyth_group_end = over_7000
                      twentyth_group_lines = lines[nineteenth_group_end : twentyth_group_end]
                      twentyth_group_text = ''.join(twentyth_group_lines)
                      if i == 19 and len(index_list) == 20:
                          end = line_count + 1
                          remain_group = lines[twentyth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 20:
                      twentyoneth_group_end = over_7000
                      twentyoneth_group_lines = lines[twentyth_group_end : twentyoneth_group_end]
                      twentyoneth_group_text = ''.join(twentyoneth_group_lines)
                      if i == 20 and len(index_list) == 21:
                          end = line_count + 1
                          remain_group = lines[twentyoneth_group_end : end]
                          remain_group_text = ''.join(remain_group)

            # 分割したテキストデータの内、2つをランダムに選択してそのテキストデータからクイズを生成する
            if len(index_list)==0:
                short_data = all_data
            elif len(index_list) == 1:
                first_data = first_group_text
                second_data = remain_group_text
            elif len(index_list) == 2:
                first_data = first_group_text
                second_data = second_group_text
            else:
                data_number = random.sample(range(len(index_list)),2)
                first_data_number = data_number[0]
                second_data_number = data_number[1]

                if first_data_number == 0:
                    first_data = first_group_text
                if first_data_number == 1:
                    first_data = second_group_text
                if first_data_number == 2:
                    first_data = third_group_text
                if first_data_number == 3:
                    first_data = fourth_group_text
                if first_data_number == 4:
                    first_data = fifth_group_text
                if first_data_number == 5:
                    first_data = sixth_group_text
                if first_data_number == 6:
                    first_data = seventh_group_text
                if first_data_number == 7:
                    first_data = eighth_group_text
                if first_data_number == 8:
                    first_data = ninth_group_text
                if first_data_number == 9:
                    first_data = tenth_group_text
                if first_data_number == 10:
                    first_data = eleventh_group_text
                if first_data_number == 11:
                    first_data = twelveth_group_text
                if first_data_number == 12:
                    first_data = thirteenth_group_text
                if first_data_number == 13:
                    first_data = fourteenth_group_text
                if first_data_number == 14:
                    first_data = fifteenth_group_text
                if first_data_number == 15:
                    first_data = sixteenth_group_text
                if first_data_number == 16:
                    first_data = seventeenth_group_text
                if first_data_number == 17:
                    first_data = eighteenth_group_text
                if first_data_number == 18:
                    first_data = nineteenth_group_text
                if first_data_number == 19:
                    first_data = twentyth_group_text
                if first_data_number == 20:
                    first_data = twentyoneth_group_text

                if second_data_number == 0:
                    second_data = first_group_text
                if second_data_number == 1:
                    second_data = second_group_text
                if second_data_number == 2:
                    second_data = third_group_text
                if second_data_number == 3:
                    second_data = fourth_group_text
                if second_data_number == 4:
                    second_data = fifth_group_text
                if second_data_number == 5:
                    second_data = sixth_group_text
                if second_data_number == 6:
                    second_data = seventh_group_text
                if second_data_number == 7:
                    second_data = eighth_group_text
                if second_data_number == 8:
                    second_data = ninth_group_text
                if second_data_number == 9:
                    second_data = tenth_group_text
                if second_data_number == 10:
                    second_data = eleventh_group_text
                if second_data_number == 11:
                    second_data = twelveth_group_text
                if second_data_number == 12:
                    second_data = thirteenth_group_text
                if second_data_number == 13:
                    second_data = fourteenth_group_text
                if second_data_number == 14:
                    second_data = fifteenth_group_text
                if second_data_number == 15:
                    second_data = sixteenth_group_text
                if second_data_number == 16:
                    second_data = seventeenth_group_text
                if second_data_number == 17:
                    second_data = eighteenth_group_text
                if second_data_number == 18:
                    second_data = nineteenth_group_text
                if second_data_number == 19:
                    second_data = twentyth_group_text
                if second_data_number == 20:
                    second_data = twentyoneth_group_text

            # 選ばれた2つのテキストデータを表示
            print(first_data)
            print("------------------------------------")
            print(second_data)

            # LangChainのLLMChainを使用
            template = f"""あなたは以下の3つの問題形式で、インプットされた情報のみを使って{topic}に関する難しい問題を作成してください。単純ではなく、回答者が考えさせられるような正確な内容の良問を作成してください。""" + """必ず各問題形式について1つずつ、合計3つの問題を作成してください。必ず質問文は「Q:」から始め、答えは「A:」から始めてください。必ず解説は表示しないでください。
            1.選択肢付き問題: 質問文に続いて、選択肢A, B, C, Dの形式で4つの選択肢を含めてください。正解の選択肢も明示してください。
            例:
            Q: 問題文
            (A) 選択肢A
            (B) 選択肢B
            (C) 選択肢C
            (D) 選択肢D
            A: 正解の選択肢
            2.穴埋め問題: 質問文の中に'___'（アンダーバー3つ）を使って、解答者が埋めるべき単語やフレーズの場所を示してください。
            3.並び替え問題: 質問文の中に並び替えるべき情報を示し、解答者がそれらを正しい順序に並べ替えるように指示してください。質問文の並び替えるべき時系列情報には番号をつけてください。
            インプットされた情報:{input} """

            # プロンプトテンプレート
            prompt_template = PromptTemplate(
                                    # 入力変数
                                    input_variables = ["input"],
                                    # テンプレート
                                    template = template,
                                    # 入力変数とテンプレートの検証有無
                                    validate_template = True
                                  )
            # モデルの設定
            LLM = OpenAI(
                        # OpenAIモデル名
                        model_name = "gpt-4",
                        # 出力する単語のランダム性（0から2の範囲）
                        temperature = 1.2,
                        # 生成する単語の最大単語数
                        max_tokens = 1000,
                        # 核サンプリング：値が高いほど正確・事実に基づく回答、低いほど多様な回答を生成
                        top_p = 0,
                        # 単語の繰り返し頻度。[-2:2]の範囲で設定し値が大きいほど繰り返し回数低下
                        frequency_penalty = 0.5,
                        # 単語をもう一度使うかどうか。[-2:2]で設定し値が大きいほど再出現率が低下
                        presence_penalty  = 0,
                        # API呼出失敗時の最大リトライ回数
                        max_retries = 6
                        )

            # LLMChain
            chain = LLMChain(
                            # LLMモデル
                            llm = LLM,
                            # プロンプトテンプレート
                            prompt = prompt_template,
                            # プロンプトを表示するか否か
                            verbose = True
                            )

            # クイズを生成
            if len(index_list)==0:
                quiz_text = chain.predict(input = short_data)
                print(quiz_text)

            else:
                quiz_text_1 = chain.predict(input = first_data)
                quiz_text_2 = chain.predict(input = second_data)
                print(quiz_text_1)
                print(quiz_text_2)
        
        else:
            # 文字列を250トークンで切り捨てる
            def check_and_split_text(text, max_tokens=250):
                            tokenized_text = tokenizer.encode(text)
                            if len(tokenized_text) > max_tokens:
                                tokenized_text = tokenized_text[:max_tokens]
                                text = tokenizer.decode(tokenized_text)
                            return text


            # トピックについての情報を2つのサイトから収集する。Wikipediaが検索結果にある場合、Wikipediaから必ず収集する。
            search_query = f"{topic}とは"

            # 上位10件取得する
            url_data = search_google(search_query, api_key, cx)
            for index, item in enumerate(url_data.get("items", [])):
                print(f"{index + 1}. {item['title']}\n{item['link']}\n")

            # 上位5件の検索結果のWebサイトの文章を取得
            documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]

            # Noneの要素を除外（取得に失敗した場合）
            documents = [doc for doc in documents if doc is not None]

            # 検索キーワードと各Webサイトの文章の適合率を求める
            relevance_scores = calculate_relevance_score_st(search_query, documents)
                            
            # Wikipediaのサイトがある場合、適合率を100にする
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    relevance_scores[i] = 100

            # 検索結果順位とそのサイトの適合率を表示
            for i, score in enumerate(relevance_scores):
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            # 適合率が高い上位2つのインデックスを取得
            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[0]
            top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[1]

            #サイトのテキストを取得
            genre_top1_result = documents[top_1_indices]
            genre_top2_result = documents[top_2_indices]

            # リストの要素を空白で連結し、情報を変数に格納
            final_genre1_data = " " .join(genre_top1_result)
            final_genre2_data = " " .join(genre_top2_result)

            # 文字列をそれぞれ250文字で切り捨てる
            shortened1_text = final_genre1_data[:250]
            shortened2_text = final_genre2_data[:250]

            #文字列をそれぞれ250トークンで切り捨てる
            f_final1_genre_data = check_and_split_text(shortened1_text)
            f_final2_genre_data = check_and_split_text(shortened2_text)
            f_final_genre_data = f_final1_genre_data + f_final2_genre_data
            print(f_final_genre_data)


            # スプレイピングしたWikipediaの情報をもとにGPT-4にジャンルを選択させる
            template = """必ず<ジャンルとジャンルキーワード>の中から、キーワードと情報に当てはまるジャンルとジャンルキーワードを最低1つ、最大2つ選択して、必ず型と同じ形式で示してください。
            キーワード：{topic}
            情報：{f_final_genre_data}

            <型>
            選択したジャンル：
            学術・知識
            ジャンルキーワード：
            「{topic} 面白い問題」

            <ジャンルとジャンルキーワード>
            学術:
            「{topic} 面白い問題」
            映画:
            「{topic} ストーリー」
            音楽:
            「{topic} とは」
            テレビ番組:
            「{topic} どんな番組」
            アニメ・マンガ:
              「{topic} あらすじ」
            芸能人・俳優・声優・セレブリティ:
              「{topic} プロフィール」
            アーティスト：
            　「{topic} おすすめの曲」
            キャラクター：
            「{topic} プロフィール」
            Youtuber:
            「{topic} おすすめ動画」
            スポーツ:
            「{topic} とは」
            料理・食べ物:
            「{topic} とは」
            旅行:
            「{topic} 有名観光地」
            ファッション:
            「{topic} どんな商品があるか」
            健康・フィットネス:
            「{topic} とは」
            自然・環境:
            「{topic} とは」
            テクノロジー:
            「{topic} とは」
            ゲーム：
            「{topic} ゲーム」
            その他：
            「{topic} とは」"""

            prompt_template = PromptTemplate(
                                    input_variables = ["topic", "f_final_genre_data"],
                                    template = template,
                                    validate_template = True,
                                    )


            LLM = OpenAI(
                        model_name = "gpt-4",
                        temperature = 1.2,
                        max_tokens = 1000,
                        top_p = 0,
                        frequency_penalty = 0.5,
                        presence_penalty  = 0,
                        max_retries = 6
                        )


            # LLMChain
            chain = LLMChain(
                            llm = LLM,
                            prompt = prompt_template,
                            verbose = True
                            )


            genre = chain.run({"topic":f"{topic}", "f_final_genre_data":f"{f_final_genre_data}"})
            print(genre)


            # トピックについての情報を収集
            # 選択したジャンルが1つ・・・適合率上位1つのサイトの文章とWikipediaの文章を取得
            if genre.count("\n") + 1 == 4:

                # 文字列を行ごとに分割
                lines = genre.split('\n')

                # 4行目を取得
                keyword_1_1 = lines[3]
                keyword_1_1 = keyword_1_1.replace("「", "").replace("」", "")

                # ジャンルキーワードを表示
                print(keyword_1_1)

                # ジャンルキーワードで検索
                search_query = keyword_1_1
                print(f"検索しています：{search_query}")

                # 上位10件取得する
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                # 選択されたジャンルキーワードでのWikioedia以外の適合率1位のサイトをスプレイピングする
                search_query = keyword_1_1
                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]
                relevance_scores = calculate_relevance_score_st(search_query, documents)

                # Wikipediaのサイトがある場合、適合率を0にする
                for i, item in enumerate(url_data.get("items", [])):
                    if i >= len(relevance_scores):
                        break
                    if "ja.wikipedia.org" in item['link']:
                        if i >= len(relevance_scores):
                            break
                        relevance_scores[i] = 0

                for i, score in enumerate(relevance_scores):
                    #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
                    text = documents[i]
                    tokens_all = tokenizer.tokenize(text)
                    if len(tokens_all) <= 1000:
                        score -= 0.12
                        relevance_scores[i] -= 0.12
                    elif 1000 < len(tokens_all) <= 3000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06
                    elif 4530 < len(tokens_all) <= 5830:
                        score += 0.06
                        relevance_scores[i] += 0.06
                    elif len(tokens_all) >= 7000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06

                    print(f"Webサイト {i + 1} の適合率: {score:.2f}")
                top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                genre_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
                genre_top_result = " " .join(genre_top_result)
                print(genre_top_result)

                #トピックに関するWikipediaのサイトをスプレイピングする
                keyword_wiki_1 = f"{topic} Wikipedia"
                search_query = keyword_wiki_1
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")
                search_query = keyword_wiki_1
                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]
                relevance_scores = calculate_relevance_score_st(search_query, documents)

                # Wikipediaのサイトがある場合、適合率を100にする
                for i, item in enumerate(url_data.get("items", [])):
                    if i >= len(relevance_scores):
                        break
                    if "ja.wikipedia.org" in item['link']:
                        relevance_scores[i] = 100

                for i, score in enumerate(relevance_scores):
                    #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
                    text = documents[i]
                    tokens_all = tokenizer.tokenize(text)
                    if len(tokens_all) <= 1000:
                        score -= 0.12
                        relevance_scores[i] -= 0.12
                    elif 1000 < len(tokens_all) <= 3000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06
                    elif 4530 < len(tokens_all) <= 5830:
                        score += 0.06
                        relevance_scores[i] += 0.06
                    elif len(tokens_all) >= 7000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06

                    print(f"Webサイト {i + 1} の適合率: {score:.2f}")
                top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                wiki_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
                wiki_top_result = " " .join(wiki_top_result)
                print(wiki_top_result)


            # トピックについての情報を収集
            #キーワードが2つ・・・適合率上位1つのサイトの文章とWikipediaの文章を取得
            if genre.count("\n") + 1 == 8 or genre.count("\n") + 1 ==9:
                lines = genre.split('\n')
                keyword_2_1 = lines[3]
                keyword_2_1 = keyword_2_1.replace("「", "").replace("」", "")
                print(keyword_2_1)

                search_query = keyword_2_1
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]
                relevance_scores = calculate_relevance_score_st(search_query, documents)

                # Wikipediaのサイトがある場合、適合率を0にする
                for i, item in enumerate(url_data.get("items", [])):
                    if i >= len(relevance_scores):
                        break
                    if "ja.wikipedia.org" in item['link']:
                        if i >= len(relevance_scores):
                            break
                        relevance_scores[i] = 0

                for i, score in enumerate(relevance_scores):
                    #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
                    text = documents[i]
                    tokens_all = tokenizer.tokenize(text)
                    if len(tokens_all) <= 1000:
                        score -= 0.12
                        relevance_scores[i] -= 0.12
                    elif 1000 < len(tokens_all) <= 3000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06
                    elif 4530 < len(tokens_all) <= 5830:
                        score += 0.06
                        relevance_scores[i] += 0.06
                    elif len(tokens_all) >= 7000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06

                    print(f"Webサイト {i + 1} の適合率: {score:.2f}")
                top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                genre_1_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
                genre_1_top_result = " " .join(genre_1_top_result)

                # キーワードがある行を取得
                if genre.count("\n") + 1 == 8:
                    keyword_2_2 = lines[7]
                elif genre.count("\n") + 1 ==9:
                    keyword_2_2 = lines[8]

                keyword_2_2 = keyword_2_2.replace("「", "").replace("」", "")

                #2つ目のジャンルキーワードを表示
                print(keyword_2_2)

                # 2つ目のジャンルキーワードでWikipedia以外の適合率1位のサイトをスプレイピングする
                search_query = keyword_2_2
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]
                relevance_scores = calculate_relevance_score_st(search_query, documents)
                for i, item in enumerate(url_data.get("items", [])):
                    if i >= len(relevance_scores):
                        break
                    if "ja.wikipedia.org" in item['link']:
                        relevance_scores[i] = 0

                for i, score in enumerate(relevance_scores):
                    #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
                    text = documents[i]
                    tokens_all = tokenizer.tokenize(text)
                    if len(tokens_all) <= 1000:
                        score -= 0.12
                        relevance_scores[i] -= 0.12
                    elif 1000 < len(tokens_all) <= 3000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06
                    elif 4530 < len(tokens_all) <= 5830:
                        score += 0.06
                        relevance_scores[i] += 0.06
                    elif len(tokens_all) >= 7000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06

                    print(f"Webサイト {i + 1} の適合率: {score:.2f}")        
                top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                genre_2_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
                genre_2_top_result = " ".join(genre_2_top_result)
                print(genre_2_top_result)

                keyword_wiki = f"{topic} Wikipedia"
                search_query = keyword_wiki
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]
                relevance_scores = calculate_relevance_score_st(search_query, documents)
                for i, item in enumerate(url_data.get("items", [])):
                    if i >= len(relevance_scores):
                        break
                    if "ja.wikipedia.org" in item['link']:
                        relevance_scores[i] = 100

                for i, score in enumerate(relevance_scores):
                    #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
                    text = documents[i]
                    tokens_all = tokenizer.tokenize(text)
                    if len(tokens_all) <= 1000:
                        score -= 0.12
                        relevance_scores[i] -= 0.12
                    elif 1000 < len(tokens_all) <= 3000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06
                    elif 4530 < len(tokens_all) <= 5830:
                        score += 0.06
                        relevance_scores[i] += 0.06
                    elif len(tokens_all) >= 7000:
                        score -= 0.06
                        relevance_scores[i] -= 0.06

                    print(f"Webサイト {i + 1} の適合率: {score:.2f}")
                top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                wiki_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
                wiki_top_result = " " .join(wiki_top_result)
                print(wiki_top_result)


            # 合計トークン数を16000トークンに制限
            import unicodedata

            if genre.count("\n") + 1 == 4:
                text_all = genre_top_result
                lines = text_all.replace('\n', '').replace("。", "。\n").replace(".",".\n").split('\n')
                lines = [line for line in lines if line.strip()]
                texts = " " .join(lines)
                tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

                tokens_all = tokenizer.tokenize(texts)

                if len(tokens_all) > 8000:
                    line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                    total = 0
                    index_list = []
                    for i, length in enumerate(line_lengths):
                        total += length
                        if total > 8000:
                            index_list.append(i)
                            total = line_lengths[i]

                    for i in range(len(index_list)):
                        over_8000 = index_list[i]
                        if i == 0:
                            first_group_end = over_8000
                            first_group_lines = lines[0 : first_group_end]
                            genre_top_group_text_8000 = '\n'.join(first_group_lines)

                else:
                    genre_top_group_text_8000 = texts

                text_all = wiki_top_result
                lines = text_all.replace('\n', '').replace("。", "。\n").replace(".",".\n").split('\n')
                lines = [line for line in lines if line.strip()]
                texts = " " .join(lines)
                tokens_all = tokenizer.tokenize(texts)

                if len(tokens_all) > 8000:
                    line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                    total = 0
                    index_list = []
                    for i, length in enumerate(line_lengths):
                        total += length
                        if total > 8000:
                            index_list.append(i)
                            total = line_lengths[i]

                    for i in range(len(index_list)):
                        over_8000 = index_list[i]
                        if i == 0:
                            first_group_end = over_8000
                            first_group_lines = lines[0 : first_group_end]
                            wiki_top_group_text_8000 = '\n'.join(first_group_lines)

                else:
                    wiki_top_group_text_8000 = texts

            if genre.count("\n") + 1 == 8 or genre.count("\n") + 1 ==9:
                text_all = genre_1_top_result
                lines = text_all.replace('\n', '').replace("。", "。\n").replace(".",".\n").split('\n')
                lines = [line for line in lines if line.strip()]
                texts = " " .join(lines)
                tokens_all = tokenizer.tokenize(texts)

                if len(tokens_all) > 5330:
                    line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                    total = 0
                    index_list = []
                    for i, length in enumerate(line_lengths):
                        total += length
                        if total > 5330:
                            index_list.append(i)
                            total = line_lengths[i]

                    for i in range(len(index_list)):
                        over_5330 = index_list[i]
                        if i == 0:
                            first_group_end = over_5330
                            first_group_lines = lines[0 : first_group_end]
                            genre_1_top_group_text_5330 = '\n'.join(first_group_lines)

                else:
                    genre_1_top_group_text_5330 = texts

                text_all = genre_2_top_result
                text = text_all.replace('\n', '')
                lines = text.replace("。", "。\n").replace(".",".\n").split('\n')
                texts = " " .join(lines)
                tokens_all = tokenizer.tokenize(texts)

                if len(tokens_all) > 5330:
                    line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                    total = 0
                    index_list = []
                    for i, length in enumerate(line_lengths):
                        total += length
                        if total > 5330:
                            index_list.append(i)
                            total = line_lengths[i]

                    for i in range(len(index_list)):
                        over_5330 = index_list[i]
                        if i == 0:
                            first_group_end = over_5330
                            first_group_lines = lines[0 : first_group_end]
                            genre_2_top_group_text_5330 = '\n'.join(first_group_lines)

                else:
                    genre_2_top_group_text_5330 = texts

                text_all = wiki_top_result
                text = text_all.replace('\n', '')
                lines = text.replace("。", "。\n").replace(".",".\n").split('\n')
                texts = " " .join(lines)
                tokens_all = tokenizer.tokenize(texts)

                if len(tokens_all) > 5330:
                    line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                    total = 0
                    index_list = []
                    for i, length in enumerate(line_lengths):
                        total += length
                        if total > 0:
                            index_list.append(i)
                            total = line_lengths[i]

                    for i in range(len(index_list)):
                        over_5330 = index_list[i]
                        if i == 0:
                            first_group_end = over_5330
                            first_group_lines = lines[0 : first_group_end]
                            wiki_top_group_text_5330 = '\n'.join(first_group_lines)
                else:
                    wiki_top_group_text_5330 = texts


            # データの結合
            if genre.count("\n") + 1 == 4:
                subkey10_data = genre_top_group_text_8000 + wiki_top_group_text_8000
            else:
                subkey10_data = genre_1_top_group_text_5330 + genre_2_top_group_text_5330 + wiki_top_group_text_5330


            # サブキーワード生成
            template = """{topic}に関連する{subkey10_data}の情報から、最も重要な{topic}に関する人物や物体を10個以下で挙げてください。結果は、各行に一つずつ人物名または名称を記載してください。
            <例>
            キーワード1つ目
            キーワード2つ目
            キーワード3つ目
            ・
            ・
            ・
            ・
            ・
            ・
            キーワード10つ目"""

            prompt_template = PromptTemplate(
                                    input_variables = ["topic", "subkey10_data"],
                                    template = template,
                                    validate_template = True
                                    )


            LLM = OpenAI(
                        model_name = "gpt-3.5-turbo-16k",
                        temperature = 1.2,
                        max_tokens = 1000,
                        top_p = 0,
                        frequency_penalty = 0.5,
                        presence_penalty  = 0,
                        max_retries = 6
                        )


            # LLMChain
            chain = LLMChain(
                            llm = LLM,
                            prompt = prompt_template,
                            verbose = True
                            )


            subkey10 = chain.run({"topic":f"{topic}", "subkey10_data":f"{subkey10_data}"})
            subkey10 = subkey10.replace("- ", "").replace("・","").replace("1.","").replace("2.","").replace("3.","").replace("4.","").replace("5.","").replace("6.","").replace("7.","").replace("8.","").replace("9.","").replace("10.","")    
            print(subkey10)


            # キーワードを取り出すために行ごとに分割
            subkey10_lines = subkey10.split('\n')
            print(subkey10_lines)


            # サブキーワード1つ目で検索
            if len(subkey10_lines) >= 1:
                subkey10_1 = subkey10_lines[0]
                if "（" in subkey10_1:
                  end_index = subkey10_1.find("（")
                  subkey10_1 = subkey10_1[:end_index]

                search_query = f"{topic} {subkey10_1}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_1_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_1_data = " " .join(subkey10_1_data)
                    print(subkey10_1_data)

                else:
                    subkey10_1_data = ""


            # サブキーワード2つ目で検索
            if len(subkey10_lines) >= 2:
                subkey10_2 = subkey10_lines[1]
                if "（" in subkey10_2:
                  end_index = subkey10_2.find("（")
                  subkey10_2 = subkey10_2[:end_index]

                search_query = f"{topic} {subkey10_2}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_2_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_2_data = " " .join(subkey10_2_data)
                    print(subkey10_2_data)

                else:
                    subkey10_2_data = ""


            # サブキーワード3つ目で検索
            if len(subkey10_lines) >= 3:
                subkey10_3 = subkey10_lines[2]
                if "（" in subkey10_3:
                  end_index = subkey10_3.find("（")
                  subkey10_3 = subkey10_3[:end_index]

                search_query = f"{topic} {subkey10_3}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_3_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_3_data = " " .join(subkey10_3_data)
                    print(subkey10_3_data)

                else:
                    subkey10_3_data = ""


            # サブキーワード4つ目で検索
            if len(subkey10_lines) >= 4:
                subkey10_4 = subkey10_lines[3]
                if "（" in subkey10_4:
                  end_index = subkey10_4.find("（")
                  subkey10_4 = subkey10_4[:end_index]

                search_query = f"{topic} {subkey10_4}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_4_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_4_data = " " .join(subkey10_4_data)
                    print(subkey10_4_data)

                else:
                    subkey10_4_data = ""


            # サブキーワード5つ目で検索
            if len(subkey10_lines) >= 5:
                subkey10_5 = subkey10_lines[4]
                if "（" in subkey10_5:
                  end_index = subkey10_5.find("（")
                  subkey10_5 = subkey10_5[:end_index]

                search_query = f"{topic} {subkey10_5}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_5_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_5_data = " " .join(subkey10_5_data)
                    print(subkey10_5_data)

                else:
                    subkey10_5_data = ""


            # サブキーワード6つ目で検索
            if len(subkey10_lines) >= 6:
                subkey10_6 = subkey10_lines[5]
                if "（" in subkey10_6:
                  end_index = subkey10_6.find("（")
                  subkey10_6 = subkey10_6[:end_index]

                search_query = f"{topic} {subkey10_6}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_6_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_6_data = " " .join(subkey10_6_data)
                    print(subkey10_6_data)

                else:
                    subkey10_6_data = ""


            # サブキーワード7つ目で検索
            if len(subkey10_lines) >= 7:
                subkey10_7 = subkey10_lines[6]
                if "（" in subkey10_7:
                  end_index = subkey10_7.find("（")
                  subkey10_7 = subkey10_7[:end_index]

                search_query = f"{topic} {subkey10_7}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_7_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_7_data = " " .join(subkey10_7_data)
                    print(subkey10_7_data)

                else:
                    subkey10_7_data = ""


            # サブキーワード8つ目で検索
            if len(subkey10_lines) >= 8:
                subkey10_8 = subkey10_lines[7]
                if "（" in subkey10_8:
                  end_index = subkey10_8.find("（")
                  subkey10_8 = subkey10_8[:end_index]

                search_query = f"{topic} {subkey10_8}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_8_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_8_data = " " .join(subkey10_8_data)
                    print(subkey10_8_data)

                else:
                    subkey10_8_data = ""


            # サブキーワード9つ目で検索
            if len(subkey10_lines) >= 9:
                subkey10_9 = subkey10_lines[8]
                if "（" in subkey10_9:
                  end_index = subkey10_9.find("（")
                  subkey10_9 = subkey10_9[:end_index]

                search_query = f"{topic} {subkey10_9}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_9_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_9_data = " " .join(subkey10_9_data)
                    print(subkey10_9_data)

                else:
                    subkey10_9_data = ""


            # サブキーワード10つ目で検索
            if len(subkey10_lines) >= 10:
                subkey10_10 = subkey10_lines[9]
                if "（" in subkey10_10:
                  end_index = subkey10_10.find("（")
                  subkey10_10 = subkey10_10[:end_index]

                search_query = f"{topic} {subkey10_10}とは"
                print(f"検索しています：{search_query}")
                url_data = search_google(search_query, api_key, cx)
                for index, item in enumerate(url_data.get("items", [])):
                    print(f"{index + 1}. {item['title']}\n{item['link']}\n")

                documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
                documents = [doc for doc in documents if doc is not None]

                if len(documents)!= 0:
                    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    for i, item in enumerate(url_data.get("items", [])):
                        if "ja.wikipedia.org" in item['link']:
                            if i >= len(relevance_scores):
                                break
                            else:
                                relevance_scores[i] = 0

                    for i, score in enumerate(relevance_scores):
                        text = documents[i]
                        tokens_all = tokenizer.tokenize(text)
                        if len(tokens_all) <= 1000:
                            score -= 0.12
                            relevance_scores[i] -= 0.12
                        elif 1000 < len(tokens_all) <= 3000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        elif 6000 < len(tokens_all) <= 7300:
                            score += 0.06
                            relevance_scores[i] += 0.06
                        elif len(tokens_all) >= 10000:
                            score -= 0.06
                            relevance_scores[i] -= 0.06
                        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

                    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
                    subkey10_10_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
                    subkey10_10_data = " " .join(subkey10_10_data)
                    print(subkey10_10_data)

                else:
                    subkey10_10_data = ""


            # データの結合
            if genre.count("\n") + 1 == 4:
                if len(subkey10_lines)==1:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data
                if len(subkey10_lines)==2:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data
                if len(subkey10_lines)==3:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data
                if len(subkey10_lines)==4:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data
                if len(subkey10_lines)==5:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data
                if len(subkey10_lines)==6:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data
                if len(subkey10_lines)==7:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data
                if len(subkey10_lines)==8:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data
                if len(subkey10_lines)==9:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data
                if len(subkey10_lines)==10:
                    all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data + subkey10_10_data

            if genre.count("\n") + 1 == 8 or genre.count("\n") + 1 ==9:
                if len(subkey10_lines)==1:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data
                if len(subkey10_lines)==2:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data
                if len(subkey10_lines)==3:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data
                if len(subkey10_lines)==4:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data
                if len(subkey10_lines)==5:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data
                if len(subkey10_lines)==6:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data
                if len(subkey10_lines)==7:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data
                if len(subkey10_lines)==8:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data
                if len(subkey10_lines)==9:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data
                if len(subkey10_lines)==10:
                    all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data + subkey10_10_data

            print(all_data)


            # ファイルに書き込んで保存
            if os.path.exists("test.txt"):
                with open("test.txt", "a", encoding="utf-8") as f:

                    # 現在の日付を取得
                    current_date = datetime.datetime.now()
                    text = f"{topic}\n始め\n{all_data}\n{topic}\n終わり"
                    f.write(f"\n{current_date.strftime('%Y-%m-%d %H:%M:%S')},{text}")
            else:
                with open("test.txt", "w", encoding="utf-8") as f:
                    current_date = datetime.datetime.now()
                    text = f"{topic}\n始め\n{all_data}\n{topic}\n終わり"
                    f.write(f"{current_date.strftime('%Y-%m-%d %H:%M:%S')},{text}")


            # トークン数で行ごとに分割
            import unicodedata

            text_all = all_data
            text = text_all.replace('\n', '')
            lines = text.replace("。", "。\n").replace(".",".\n").split('\n')

            # 空行を削除
            lines = [line for line in lines if line.strip() != ""]

            # 行数をカウント
            line_count = len(lines)
            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
            lines = "".join(lines)
            tokens_all = tokenizer.tokenize(text)

            if len(tokens_all) > 6800:
                line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
                total = 0
                index_list = []
                for i, length in enumerate(line_lengths):
                    total += length
                    if total > 6800:
                        index_list.append(i)
                        total =  line_lengths[i]

            else:
                index_list = []


            if len(tokens_all) > 6800:

                # 6800トークン以下になるようにデータを行ごとに分割
                for i in range(len(index_list)):
                    over_6800 = index_list[i]

                    if i == 0:
                      first_group_end = over_6800
                      first_group_lines = lines[0 : first_group_end]
                      first_group_text = ''.join(first_group_lines)
                      if i == 0 and len(index_list) == 1:
                          end = line_count + 1
                          remain_group = lines[first_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 1:
                      second_group_end = over_6800
                      second_group_lines = lines[first_group_end : second_group_end]
                      second_group_text = ''.join(second_group_lines)
                      if i == 1 and len(index_list) == 2:
                          end = line_count + 1
                          remain_group = lines[second_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 2:
                      third_group_end  = over_6800
                      third_group_lines = lines[second_group_end : third_group_end]
                      third_group_text = ''.join(third_group_lines)
                      if i == 2 and len(index_list) == 3:
                          end = line_count + 1
                          remain_group = lines[third_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 3:
                      fourth_group_end = over_6800
                      fourth_group_lines = lines[third_group_end : fourth_group_end]
                      fourth_group_text = ''.join(fourth_group_lines)
                      if i == 3 and len(index_list) == 4:
                          end = line_count + 1
                          remain_group = lines[fourth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 4:
                      fifth_group_end = over_6800
                      fifth_group_lines = lines[fourth_group_end : fifth_group_end]
                      fifth_group_text = ''.join(fifth_group_lines)
                      if i == 4 and len(index_list) == 5:
                          end = line_count + 1
                          remain_group = lines[fifth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 5:
                      sixth_group_end = over_6800
                      sixth_group_lines = lines[fifth_group_end : sixth_group_end]
                      sixth_group_text = ''.join(sixth_group_lines)
                      if i == 5 and len(index_list) == 6:
                          end = line_count + 1
                          remain_group = lines[sixth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 6:
                      seventh_group_end = over_6800
                      seventh_group_lines = lines[sixth_group_end : seventh_group_end]
                      seventh_group_text = ''.join(seventh_group_lines)
                      if i == 6 and len(index_list) == 7:
                          end = line_count + 1
                          remain_group = lines[seventh_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 7:
                      eighth_group_end = over_6800
                      eighth_group_lines = lines[seventh_group_end : eighth_group_end]
                      eighth_group_text = ''.join(eighth_group_lines)
                      if i == 7 and len(index_list) == 8:
                          end = line_count + 1
                          remain_group = lines[eighth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 8:
                      ninth_group_end = over_6800
                      ninth_group_lines = lines[eighth_group_end : ninth_group_end]
                      ninth_group_text = ''.join(ninth_group_lines)
                      if i == 8 and len(index_list) == 9:
                          end = line_count + 1
                          remain_group = lines[ninth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 9:
                      tenth_group_end = over_6800
                      tenth_group_lines = lines[ninth_group_end : tenth_group_end]
                      tenth_group_text = ''.join(tenth_group_lines)
                      if i == 9 and len(index_list) == 10:
                          end = line_count + 1
                          remain_group = lines[tenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 10:
                      eleventh_group_end = over_6800
                      eleventh_group_lines = lines[tenth_group_end : eleventh_group_end]
                      eleventh_group_text = ''.join(eleventh_group_lines)
                      if i == 10 and len(index_list) == 11:
                          end = line_count + 1
                          remain_group = lines[eleventh_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 11:
                      twelveth_group_end = over_6800
                      twelveth_group_lines = lines[eleventh_group_end : twelveth_group_end]
                      twelveth_group_text = ''.join(twelveth_group_lines)
                      if i == 11 and len(index_list) == 12:
                          end = line_count + 1
                          remain_group = lines[twelveth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 12:
                      thirteenth_group_end = over_6800
                      thirteenth_group_lines = lines[twelveth_group_end : thirteenth_group_end]
                      thirteenth_group_text = ''.join(thirteenth_group_lines)
                      if i == 12 and len(index_list) == 13:
                          end = line_count + 1
                          remain_group = lines[thirteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 13:
                      fourteenth_group_end = over_6800
                      fourteenth_group_lines = lines[thirteenth_group_end : fourteenth_group_end]
                      fourteenth_group_text = ''.join(fourteenth_group_lines)
                      if i == 13 and len(index_list) == 14:
                          end = line_count + 1
                          remain_group = lines[fourteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 14:
                      fifteenth_group_end = over_6800
                      fifteenth_group_lines = lines[fourteenth_group_end : fifteenth_group_end]
                      fifteenth_group_text = ''.join(fifteenth_group_lines)
                      if i == 14 and len(index_list) == 15:
                          end = line_count + 1
                          remain_group = lines[fifteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 15:
                      sixteenth_group_end = over_6800
                      sixteenth_group_lines = lines[fifteenth_group_end : sixteenth_group_end]
                      sixteenth_group_text = ''.join(sixteenth_group_lines)
                      if i == 15 and len(index_list) == 16:
                          end = line_count + 1
                          remain_group = lines[sixteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 16:
                      seventeenth_group_end = over_6800
                      seventeenth_group_lines = lines[sixteenth_group_end : seventeenth_group_end]
                      seventeenth_group_text = ''.join(seventeenth_group_lines)
                      if i == 16 and len(index_list) == 17:
                          end = line_count + 1
                          remain_group = lines[seventeenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 17:
                      eighteenth_group_end = over_6800
                      eighteenth_group_lines = lines[seventeenth_group_end : eighteenth_group_end]
                      eighteenth_group_text = ''.join(eighteenth_group_lines)
                      if i == 17 and len(index_list) == 18:
                          end = line_count + 1
                          remain_group = lines[eighteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 18:
                      nineteenth_group_end = over_6800
                      nineteenth_group_lines = lines[eighteenth_group_end : nineteenth_group_end]
                      nineteenth_group_text = ''.join(nineteenth_group_lines)
                      if i == 18 and len(index_list) == 19:
                          end = line_count + 1
                          remain_group = lines[nineteenth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 19:
                      twentyth_group_end = over_6800
                      twentyth_group_lines = lines[nineteenth_group_end : twentyth_group_end]
                      twentyth_group_text = ''.join(twentyth_group_lines)
                      if i == 19 and len(index_list) == 20:
                          end = line_count + 1
                          remain_group = lines[twentyth_group_end : end]
                          remain_group_text = ''.join(remain_group)

                    if i == 20:
                      twentyoneth_group_end = over_6800
                      twentyoneth_group_lines = lines[twentyth_group_end : twentyoneth_group_end]
                      twentyoneth_group_text = ''.join(twentyoneth_group_lines)
                      if i == 20 and len(index_list) == 21:
                          end = line_count + 1
                          remain_group = lines[twentyoneth_group_end : end]
                          remain_group_text = ''.join(remain_group)


            # 使用するデータをランダム選択
            if len(index_list)==0:
                short_data = all_data

            elif len(index_list) == 1:
                first_data = first_group_text
                second_data = remain_group_text

            elif len(index_list) == 2:
                first_data = first_group_text
                second_data = second_group_text

            else:
                data_number = random.sample(range(len(index_list)),2)
                first_data_number = data_number[0]
                second_data_number = data_number[1]

                if first_data_number == 0:
                    first_data = first_group_text
                if first_data_number == 1:
                    first_data = second_group_text
                if first_data_number == 2:
                    first_data = third_group_text
                if first_data_number == 3:
                    first_data = fourth_group_text
                if first_data_number == 4:
                    first_data = fifth_group_text
                if first_data_number == 5:
                    first_data = sixth_group_text
                if first_data_number == 6:
                    first_data = seventh_group_text
                if first_data_number == 7:
                    first_data = eighth_group_text
                if first_data_number == 8:
                    first_data = ninth_group_text
                if first_data_number == 9:
                    first_data = tenth_group_text
                if first_data_number == 10:
                    first_data = eleventh_group_text
                if first_data_number == 11:
                    first_data = twelveth_group_text
                if first_data_number == 12:
                    first_data = thirteenth_group_text
                if first_data_number == 13:
                    first_data = fourteenth_group_text
                if first_data_number == 14:
                    first_data = fifteenth_group_text
                if first_data_number == 15:
                    first_data = sixteenth_group_text
                if first_data_number == 16:
                    first_data = seventeenth_group_text
                if first_data_number == 17:
                    first_data = eighteenth_group_text
                if first_data_number == 18:
                    first_data = nineteenth_group_text
                if first_data_number == 19:
                    first_data = twentyth_group_text
                if first_data_number == 20:
                    first_data = twentyoneth_group_text

                if second_data_number == 0:
                    second_data = first_group_text
                if second_data_number == 1:
                    second_data = second_group_text
                if second_data_number == 2:
                    second_data = third_group_text
                if second_data_number == 3:
                    second_data = fourth_group_text
                if second_data_number == 4:
                    second_data = fifth_group_text
                if second_data_number == 5:
                    second_data = sixth_group_text
                if second_data_number == 6:
                    second_data = seventh_group_text
                if second_data_number == 7:
                    second_data = eighth_group_text
                if second_data_number == 8:
                    second_data = ninth_group_text
                if second_data_number == 9:
                    second_data = tenth_group_text
                if second_data_number == 10:
                    second_data = eleventh_group_text
                if second_data_number == 11:
                    second_data = twelveth_group_text
                if second_data_number == 12:
                    second_data = thirteenth_group_text
                if second_data_number == 13:
                    second_data = fourteenth_group_text
                if second_data_number == 14:
                    second_data = fifteenth_group_text
                if second_data_number == 15:
                    second_data = sixteenth_group_text
                if second_data_number == 16:
                    second_data = seventeenth_group_text
                if second_data_number == 17:
                    second_data = eighteenth_group_text
                if second_data_number == 18:
                    second_data = nineteenth_group_text
                if second_data_number == 19:
                    second_data = twentyth_group_text
                if second_data_number == 20:
                    second_data = twentyoneth_group_text

            print(first_data)
            print("------------------------------------")
            print(second_data)


            # クイズ生成
            from langchain.chains import ConversationChain
            from langchain import PromptTemplate

            template = f"""あなたは以下の3つの問題形式で、インプットされた情報のみを使って{topic}に関する難しい問題を作成してください。単純ではなく、回答者が考えさせられるような正確な内容の良問を作成してください。""" + """必ず各問題形式について1つずつ、合計3つの問題を作成してください。必ず質問文は「Q:」から始め、答えは「A:」から始めてください。必ず解説は表示しないでください。
                        1.選択肢付き問題: 質問文に続いて、選択肢A, B, C, Dの形式で4つの選択肢を含めてください。正解の選択肢も明示してください。
                        例:
                        Q: 問題文
                        (A) 選択肢A
                        (B) 選択肢B
                        (C) 選択肢C
                        (D) 選択肢D
                        A: 正解の選択肢
                        2.穴埋め問題: 質問文の中に'___'（アンダーバー3つ）を使って、解答者が埋めるべき単語やフレーズの場所を示してください。答えは改行して表示してください。
                        3.並び替え問題: 質問文の中に並び替えるべき情報を示し、解答者にそれらを正しい順序に並べ替えるよう指示してください。質問文の並び替えるべき時系列情報には番号をつけてください。答えは改行せずに表示してください。必ず解説は表示しないでください。
                        インプットされた情報:{input} """

            # プロンプトテンプレート
            prompt_template = PromptTemplate(
                                    input_variables   = ["input"],
                                    template          = template,
                                    validate_template = True,
                                    )


            from langchain.llms import OpenAI
            LLM = OpenAI(
                        model_name = "gpt-4",
                        temperature = 1.2,
                        max_tokens = 1000,
                        top_p = 0,
                        frequency_penalty = 0.5,
                        presence_penalty  = 0,
                        n = 1,
                        max_retries = 6,
                        )


            chain = LLMChain(
                            llm     = LLM,
                            prompt  = prompt_template,
                            verbose = True,
                            )


            if len(index_list)==0:
                quiz_text = chain.predict(input = short_data)
                print(quiz_text)

            else:
                quiz_text_1 = chain.predict(input = first_data)
                quiz_text_2 = chain.predict(input = second_data)
                print(quiz_text_1)
                print(quiz_text_2)

                
else:
    # 文字列を250トークンで切り捨てる
    def check_and_split_text(text, max_tokens=250):
                    tokenized_text = tokenizer.encode(text)
                    if len(tokenized_text) > max_tokens:
                        tokenized_text = tokenized_text[:max_tokens]
                        text = tokenizer.decode(tokenized_text)
                    return text


    # トピックについての情報を2つのサイトから収集する。Wikipediaが検索結果にある場合、Wikipediaから必ず収集する。
    search_query = f"{topic}とは"

    # 上位10件取得する
    url_data = search_google(search_query, api_key, cx)
    for index, item in enumerate(url_data.get("items", [])):
        print(f"{index + 1}. {item['title']}\n{item['link']}\n")

    # 上位5件の検索結果のWebサイトの文章を取得
    documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]

    # Noneの要素を除外（取得に失敗した場合）
    documents = [doc for doc in documents if doc is not None]

    # 検索キーワードと各Webサイトの文章の適合率を求める
    relevance_scores = calculate_relevance_score_st(search_query, documents)
                    
    # Wikipediaのサイトがある場合、適合率を100にする
    for i, item in enumerate(url_data.get("items", [])):
        if "ja.wikipedia.org" in item['link']:
            relevance_scores[i] = 100

    # 検索結果順位とそのサイトの適合率を表示
    for i, score in enumerate(relevance_scores):
        print(f"Webサイト {i + 1} の適合率: {score:.2f}")

    # 適合率が高い上位2つのインデックスを取得
    top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[0]
    top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[1]

    #サイトのテキストを取得
    genre_top1_result = documents[top_1_indices]
    genre_top2_result = documents[top_2_indices]

    # リストの要素を空白で連結し、情報を変数に格納
    final_genre1_data = " " .join(genre_top1_result)
    final_genre2_data = " " .join(genre_top2_result)

    # 文字列をそれぞれ250文字で切り捨てる
    shortened1_text = final_genre1_data[:250]
    shortened2_text = final_genre2_data[:250]

    #文字列をそれぞれ250トークンで切り捨てる
    f_final1_genre_data = check_and_split_text(shortened1_text)
    f_final2_genre_data = check_and_split_text(shortened2_text)
    f_final_genre_data = f_final1_genre_data + f_final2_genre_data
    print(f_final_genre_data)


    # スプレイピングしたWikipediaの情報をもとにGPT-4にジャンルを選択させる
    template = """必ず<ジャンルとジャンルキーワード>の中から、キーワードと情報に当てはまるジャンルとジャンルキーワードを最低1つ、最大2つ選択して、必ず型と同じ形式で示してください。
    キーワード：{topic}
    情報：{f_final_genre_data}

    <型>
    選択したジャンル：
    学術・知識
    ジャンルキーワード：
    「{topic} 面白い問題」

    <ジャンルとジャンルキーワード>
    学術:
    「{topic} 面白い問題」
    映画:
    「{topic} ストーリー」
    音楽:
    「{topic} とは」
    テレビ番組:
    「{topic} どんな番組」
    アニメ・マンガ:
      「{topic} あらすじ」
    芸能人・俳優・声優・セレブリティ:
      「{topic} プロフィール」
    アーティスト：
    　「{topic} おすすめの曲」
    キャラクター：
    「{topic} プロフィール」
    Youtuber:
    「{topic} おすすめ動画」
    スポーツ:
    「{topic} とは」
    料理・食べ物:
    「{topic} とは」
    旅行:
    「{topic} 有名観光地」
    ファッション:
    「{topic} どんな商品があるか」
    健康・フィットネス:
    「{topic} とは」
    自然・環境:
    「{topic} とは」
    テクノロジー:
    「{topic} とは」
    ゲーム：
    「{topic} ゲーム」
    その他：
    「{topic} とは」"""

    prompt_template = PromptTemplate(
                            input_variables = ["topic", "f_final_genre_data"],
                            template = template,
                            validate_template = True,
                            )


    LLM = OpenAI(
                model_name = "gpt-4",
                temperature = 1.2,
                max_tokens = 1000,
                top_p = 0,
                frequency_penalty = 0.5,
                presence_penalty  = 0,
                max_retries = 6
                )


    # LLMChain
    chain = LLMChain(
                    llm = LLM,
                    prompt = prompt_template,
                    verbose = True
                    )


    genre = chain.run({"topic":f"{topic}", "f_final_genre_data":f"{f_final_genre_data}"})
    print(genre)


    # トピックについての情報を収集
    # 選択したジャンルが1つ・・・適合率上位1つのサイトの文章とWikipediaの文章を取得
    if genre.count("\n") + 1 == 4:

        # 文字列を行ごとに分割
        lines = genre.split('\n')

        # 4行目を取得
        keyword_1_1 = lines[3]
        keyword_1_1 = keyword_1_1.replace("「", "").replace("」", "")

        # ジャンルキーワードを表示
        print(keyword_1_1)

        # ジャンルキーワードで検索
        search_query = keyword_1_1
        print(f"検索しています：{search_query}")

        # 上位10件取得する
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        # 選択されたジャンルキーワードでのWikioedia以外の適合率1位のサイトをスプレイピングする
        search_query = keyword_1_1
        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]
        relevance_scores = calculate_relevance_score_st(search_query, documents)

        # Wikipediaのサイトがある場合、適合率を0にする
        for i, item in enumerate(url_data.get("items", [])):
            if i >= len(relevance_scores):
                break
            if "ja.wikipedia.org" in item['link']:
                if i >= len(relevance_scores):
                    break
                relevance_scores[i] = 0

        for i, score in enumerate(relevance_scores):
            #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
            text = documents[i]
            tokens_all = tokenizer.tokenize(text)
            if len(tokens_all) <= 1000:
                score -= 0.12
                relevance_scores[i] -= 0.12
            elif 1000 < len(tokens_all) <= 3000:
                score -= 0.06
                relevance_scores[i] -= 0.06
            elif 4530 < len(tokens_all) <= 5830:
                score += 0.06
                relevance_scores[i] += 0.06
            elif len(tokens_all) >= 7000:
                score -= 0.06
                relevance_scores[i] -= 0.06

            print(f"Webサイト {i + 1} の適合率: {score:.2f}")
        top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
        genre_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
        genre_top_result = " " .join(genre_top_result)
        print(genre_top_result)

        #トピックに関するWikipediaのサイトをスプレイピングする
        keyword_wiki_1 = f"{topic} Wikipedia"
        search_query = keyword_wiki_1
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")
        search_query = keyword_wiki_1
        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]
        relevance_scores = calculate_relevance_score_st(search_query, documents)

        # Wikipediaのサイトがある場合、適合率を100にする
        for i, item in enumerate(url_data.get("items", [])):
            if i >= len(relevance_scores):
                break
            if "ja.wikipedia.org" in item['link']:
                relevance_scores[i] = 100

        for i, score in enumerate(relevance_scores):
            #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
            text = documents[i]
            tokens_all = tokenizer.tokenize(text)
            if len(tokens_all) <= 1000:
                score -= 0.12
                relevance_scores[i] -= 0.12
            elif 1000 < len(tokens_all) <= 3000:
                score -= 0.06
                relevance_scores[i] -= 0.06
            elif 4530 < len(tokens_all) <= 5830:
                score += 0.06
                relevance_scores[i] += 0.06
            elif len(tokens_all) >= 7000:
                score -= 0.06
                relevance_scores[i] -= 0.06

            print(f"Webサイト {i + 1} の適合率: {score:.2f}")
        top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
        wiki_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
        wiki_top_result = " " .join(wiki_top_result)
        print(wiki_top_result)


    # トピックについての情報を収集
    #キーワードが2つ・・・適合率上位1つのサイトの文章とWikipediaの文章を取得
    if genre.count("\n") + 1 == 8 or genre.count("\n") + 1 ==9:
        lines = genre.split('\n')
        keyword_2_1 = lines[3]
        keyword_2_1 = keyword_2_1.replace("「", "").replace("」", "")
        print(keyword_2_1)

        search_query = keyword_2_1
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]
        relevance_scores = calculate_relevance_score_st(search_query, documents)

        # Wikipediaのサイトがある場合、適合率を0にする
        for i, item in enumerate(url_data.get("items", [])):
            if i >= len(relevance_scores):
                break
            if "ja.wikipedia.org" in item['link']:
                if i >= len(relevance_scores):
                    break
                relevance_scores[i] = 0

        for i, score in enumerate(relevance_scores):
            #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
            text = documents[i]
            tokens_all = tokenizer.tokenize(text)
            if len(tokens_all) <= 1000:
                score -= 0.12
                relevance_scores[i] -= 0.12
            elif 1000 < len(tokens_all) <= 3000:
                score -= 0.06
                relevance_scores[i] -= 0.06
            elif 4530 < len(tokens_all) <= 5830:
                score += 0.06
                relevance_scores[i] += 0.06
            elif len(tokens_all) >= 7000:
                score -= 0.06
                relevance_scores[i] -= 0.06

            print(f"Webサイト {i + 1} の適合率: {score:.2f}")
        top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
        genre_1_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
        genre_1_top_result = " " .join(genre_1_top_result)

        # キーワードがある行を取得
        if genre.count("\n") + 1 == 8:
            keyword_2_2 = lines[7]
        elif genre.count("\n") + 1 ==9:
            keyword_2_2 = lines[8]

        keyword_2_2 = keyword_2_2.replace("「", "").replace("」", "")

        #2つ目のジャンルキーワードを表示
        print(keyword_2_2)

        # 2つ目のジャンルキーワードでWikipedia以外の適合率1位のサイトをスプレイピングする
        search_query = keyword_2_2
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]
        relevance_scores = calculate_relevance_score_st(search_query, documents)
        for i, item in enumerate(url_data.get("items", [])):
            if i >= len(relevance_scores):
                break
            if "ja.wikipedia.org" in item['link']:
                relevance_scores[i] = 0

        for i, score in enumerate(relevance_scores):
            #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
            text = documents[i]
            tokens_all = tokenizer.tokenize(text)
            if len(tokens_all) <= 1000:
                score -= 0.12
                relevance_scores[i] -= 0.12
            elif 1000 < len(tokens_all) <= 3000:
                score -= 0.06
                relevance_scores[i] -= 0.06
            elif 4530 < len(tokens_all) <= 5830:
                score += 0.06
                relevance_scores[i] += 0.06
            elif len(tokens_all) >= 7000:
                score -= 0.06
                relevance_scores[i] -= 0.06

            print(f"Webサイト {i + 1} の適合率: {score:.2f}")        
        top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
        genre_2_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
        genre_2_top_result = " ".join(genre_2_top_result)
        print(genre_2_top_result)

        keyword_wiki = f"{topic} Wikipedia"
        search_query = keyword_wiki
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]
        relevance_scores = calculate_relevance_score_st(search_query, documents)
        for i, item in enumerate(url_data.get("items", [])):
            if i >= len(relevance_scores):
                break
            if "ja.wikipedia.org" in item['link']:
                relevance_scores[i] = 100

        for i, score in enumerate(relevance_scores):
            #テキスト量が少なすぎる/多すぎるサイトを冷遇し、5330トークン付近のテキスト量のサイトを優遇する
            text = documents[i]
            tokens_all = tokenizer.tokenize(text)
            if len(tokens_all) <= 1000:
                score -= 0.12
                relevance_scores[i] -= 0.12
            elif 1000 < len(tokens_all) <= 3000:
                score -= 0.06
                relevance_scores[i] -= 0.06
            elif 4530 < len(tokens_all) <= 5830:
                score += 0.06
                relevance_scores[i] += 0.06
            elif len(tokens_all) >= 7000:
                score -= 0.06
                relevance_scores[i] -= 0.06

            print(f"Webサイト {i + 1} の適合率: {score:.2f}")
        top_2_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
        wiki_top_result = [documents[i] for i in top_2_indices if relevance_scores[i] >= 0.3]
        wiki_top_result = " " .join(wiki_top_result)
        print(wiki_top_result)


    # 合計トークン数を16000トークンに制限
    import unicodedata

    if genre.count("\n") + 1 == 4:
        text_all = genre_top_result
        lines = text_all.replace('\n', '').replace("。", "。\n").replace(".",".\n").split('\n')
        lines = [line for line in lines if line.strip()]
        texts = " " .join(lines)
        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

        tokens_all = tokenizer.tokenize(texts)

        if len(tokens_all) > 8000:
            line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
            total = 0
            index_list = []
            for i, length in enumerate(line_lengths):
                total += length
                if total > 8000:
                    index_list.append(i)
                    total = line_lengths[i]

            for i in range(len(index_list)):
                over_8000 = index_list[i]
                if i == 0:
                    first_group_end = over_8000
                    first_group_lines = lines[0 : first_group_end]
                    genre_top_group_text_8000 = '\n'.join(first_group_lines)

        else:
            genre_top_group_text_8000 = texts

        text_all = wiki_top_result
        lines = text_all.replace('\n', '').replace("。", "。\n").replace(".",".\n").split('\n')
        lines = [line for line in lines if line.strip()]
        texts = " " .join(lines)
        tokens_all = tokenizer.tokenize(texts)

        if len(tokens_all) > 8000:
            line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
            total = 0
            index_list = []
            for i, length in enumerate(line_lengths):
                total += length
                if total > 8000:
                    index_list.append(i)
                    total = line_lengths[i]

            for i in range(len(index_list)):
                over_8000 = index_list[i]
                if i == 0:
                    first_group_end = over_8000
                    first_group_lines = lines[0 : first_group_end]
                    wiki_top_group_text_8000 = '\n'.join(first_group_lines)

        else:
            wiki_top_group_text_8000 = texts

    if genre.count("\n") + 1 == 8 or genre.count("\n") + 1 ==9:
        text_all = genre_1_top_result
        lines = text_all.replace('\n', '').replace("。", "。\n").replace(".",".\n").split('\n')
        lines = [line for line in lines if line.strip()]
        texts = " " .join(lines)
        tokens_all = tokenizer.tokenize(texts)

        if len(tokens_all) > 5330:
            line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
            total = 0
            index_list = []
            for i, length in enumerate(line_lengths):
                total += length
                if total > 5330:
                    index_list.append(i)
                    total = line_lengths[i]

            for i in range(len(index_list)):
                over_5330 = index_list[i]
                if i == 0:
                    first_group_end = over_5330
                    first_group_lines = lines[0 : first_group_end]
                    genre_1_top_group_text_5330 = '\n'.join(first_group_lines)

        else:
            genre_1_top_group_text_5330 = texts

        text_all = genre_2_top_result
        text = text_all.replace('\n', '')
        lines = text.replace("。", "。\n").replace(".",".\n").split('\n')
        texts = " " .join(lines)
        tokens_all = tokenizer.tokenize(texts)

        if len(tokens_all) > 5330:
            line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
            total = 0
            index_list = []
            for i, length in enumerate(line_lengths):
                total += length
                if total > 5330:
                    index_list.append(i)
                    total = line_lengths[i]

            for i in range(len(index_list)):
                over_5330 = index_list[i]
                if i == 0:
                    first_group_end = over_5330
                    first_group_lines = lines[0 : first_group_end]
                    genre_2_top_group_text_5330 = '\n'.join(first_group_lines)

        else:
            genre_2_top_group_text_5330 = texts

        text_all = wiki_top_result
        text = text_all.replace('\n', '')
        lines = text.replace("。", "。\n").replace(".",".\n").split('\n')
        texts = " " .join(lines)
        tokens_all = tokenizer.tokenize(texts)

        if len(tokens_all) > 5330:
            line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
            total = 0
            index_list = []
            for i, length in enumerate(line_lengths):
                total += length
                if total > 0:
                    index_list.append(i)
                    total = line_lengths[i]

            for i in range(len(index_list)):
                over_5330 = index_list[i]
                if i == 0:
                    first_group_end = over_5330
                    first_group_lines = lines[0 : first_group_end]
                    wiki_top_group_text_5330 = '\n'.join(first_group_lines)
        else:
            wiki_top_group_text_5330 = texts


    # データの結合
    if genre.count("\n") + 1 == 4:
        subkey10_data = genre_top_group_text_8000 + wiki_top_group_text_8000
    else:
        subkey10_data = genre_1_top_group_text_5330 + genre_2_top_group_text_5330 + wiki_top_group_text_5330


    # サブキーワード生成
    template = """{topic}に関連する{subkey10_data}の情報から、最も重要な{topic}に関する人物や物体を10個以下で挙げてください。結果は、各行に一つずつ人物名または名称を記載してください。
    <例>
    キーワード1つ目
    キーワード2つ目
    キーワード3つ目
    ・
    ・
    ・
    ・
    ・
    ・
    キーワード10つ目"""

    prompt_template = PromptTemplate(
                            input_variables = ["topic", "subkey10_data"],
                            template = template,
                            validate_template = True
                            )


    LLM = OpenAI(
                model_name = "gpt-3.5-turbo-16k",
                temperature = 1.2,
                max_tokens = 1000,
                top_p = 0,
                frequency_penalty = 0.5,
                presence_penalty  = 0,
                max_retries = 6
                )


    # LLMChain
    chain = LLMChain(
                    llm = LLM,
                    prompt = prompt_template,
                    verbose = True
                    )


    subkey10 = chain.run({"topic":f"{topic}", "subkey10_data":f"{subkey10_data}"})
    subkey10 = subkey10.replace("- ", "").replace("・","").replace("1.","").replace("2.","").replace("3.","").replace("4.","").replace("5.","").replace("6.","").replace("7.","").replace("8.","").replace("9.","").replace("10.","")    
    print(subkey10)


    # キーワードを取り出すために行ごとに分割
    subkey10_lines = subkey10.split('\n')
    print(subkey10_lines)


    # サブキーワード1つ目で検索
    if len(subkey10_lines) >= 1:
        subkey10_1 = subkey10_lines[0]
        if "（" in subkey10_1:
          end_index = subkey10_1.find("（")
          subkey10_1 = subkey10_1[:end_index]

        search_query = f"{topic} {subkey10_1}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_1_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_1_data = " " .join(subkey10_1_data)
            print(subkey10_1_data)

        else:
            subkey10_1_data = ""


    # サブキーワード2つ目で検索
    if len(subkey10_lines) >= 2:
        subkey10_2 = subkey10_lines[1]
        if "（" in subkey10_2:
          end_index = subkey10_2.find("（")
          subkey10_2 = subkey10_2[:end_index]

        search_query = f"{topic} {subkey10_2}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_2_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_2_data = " " .join(subkey10_2_data)
            print(subkey10_2_data)

        else:
            subkey10_2_data = ""


    # サブキーワード3つ目で検索
    if len(subkey10_lines) >= 3:
        subkey10_3 = subkey10_lines[2]
        if "（" in subkey10_3:
          end_index = subkey10_3.find("（")
          subkey10_3 = subkey10_3[:end_index]

        search_query = f"{topic} {subkey10_3}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_3_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_3_data = " " .join(subkey10_3_data)
            print(subkey10_3_data)

        else:
            subkey10_3_data = ""


    # サブキーワード4つ目で検索
    if len(subkey10_lines) >= 4:
        subkey10_4 = subkey10_lines[3]
        if "（" in subkey10_4:
          end_index = subkey10_4.find("（")
          subkey10_4 = subkey10_4[:end_index]

        search_query = f"{topic} {subkey10_4}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_4_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_4_data = " " .join(subkey10_4_data)
            print(subkey10_4_data)

        else:
            subkey10_4_data = ""


    # サブキーワード5つ目で検索
    if len(subkey10_lines) >= 5:
        subkey10_5 = subkey10_lines[4]
        if "（" in subkey10_5:
          end_index = subkey10_5.find("（")
          subkey10_5 = subkey10_5[:end_index]

        search_query = f"{topic} {subkey10_5}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_5_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_5_data = " " .join(subkey10_5_data)
            print(subkey10_5_data)

        else:
            subkey10_5_data = ""


    # サブキーワード6つ目で検索
    if len(subkey10_lines) >= 6:
        subkey10_6 = subkey10_lines[5]
        if "（" in subkey10_6:
          end_index = subkey10_6.find("（")
          subkey10_6 = subkey10_6[:end_index]

        search_query = f"{topic} {subkey10_6}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_6_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_6_data = " " .join(subkey10_6_data)
            print(subkey10_6_data)

        else:
            subkey10_6_data = ""


    # サブキーワード7つ目で検索
    if len(subkey10_lines) >= 7:
        subkey10_7 = subkey10_lines[6]
        if "（" in subkey10_7:
          end_index = subkey10_7.find("（")
          subkey10_7 = subkey10_7[:end_index]

        search_query = f"{topic} {subkey10_7}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_7_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_7_data = " " .join(subkey10_7_data)
            print(subkey10_7_data)

        else:
            subkey10_7_data = ""


    # サブキーワード8つ目で検索
    if len(subkey10_lines) >= 8:
        subkey10_8 = subkey10_lines[7]
        if "（" in subkey10_8:
          end_index = subkey10_8.find("（")
          subkey10_8 = subkey10_8[:end_index]

        search_query = f"{topic} {subkey10_8}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_8_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_8_data = " " .join(subkey10_8_data)
            print(subkey10_8_data)

        else:
            subkey10_8_data = ""


    # サブキーワード9つ目で検索
    if len(subkey10_lines) >= 9:
        subkey10_9 = subkey10_lines[8]
        if "（" in subkey10_9:
          end_index = subkey10_9.find("（")
          subkey10_9 = subkey10_9[:end_index]

        search_query = f"{topic} {subkey10_9}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_9_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_9_data = " " .join(subkey10_9_data)
            print(subkey10_9_data)

        else:
            subkey10_9_data = ""


    # サブキーワード10つ目で検索
    if len(subkey10_lines) >= 10:
        subkey10_10 = subkey10_lines[9]
        if "（" in subkey10_10:
          end_index = subkey10_10.find("（")
          subkey10_10 = subkey10_10[:end_index]

        search_query = f"{topic} {subkey10_10}とは"
        print(f"検索しています：{search_query}")
        url_data = search_google(search_query, api_key, cx)
        for index, item in enumerate(url_data.get("items", [])):
            print(f"{index + 1}. {item['title']}\n{item['link']}\n")

        documents = [get_text_from_url(item['link']) for item in url_data.get("items", [])]
        documents = [doc for doc in documents if doc is not None]

        if len(documents)!= 0:
            relevance_scores = calculate_relevance_score_st(search_query, documents)
            for i, item in enumerate(url_data.get("items", [])):
                if "ja.wikipedia.org" in item['link']:
                    if i >= len(relevance_scores):
                        break
                    else:
                        relevance_scores[i] = 0

            for i, score in enumerate(relevance_scores):
                text = documents[i]
                tokens_all = tokenizer.tokenize(text)
                if len(tokens_all) <= 1000:
                    score -= 0.12
                    relevance_scores[i] -= 0.12
                elif 1000 < len(tokens_all) <= 3000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                elif 6000 < len(tokens_all) <= 7300:
                    score += 0.06
                    relevance_scores[i] += 0.06
                elif len(tokens_all) >= 10000:
                    score -= 0.06
                    relevance_scores[i] -= 0.06
                print(f"Webサイト {i + 1} の適合率: {score:.2f}")

            top_1_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)[:1]
            subkey10_10_data = [documents[i] for i in top_1_indices if relevance_scores[i] >= 0.3]
            subkey10_10_data = " " .join(subkey10_10_data)
            print(subkey10_10_data)

        else:
            subkey10_10_data = ""


    # データの結合
    if genre.count("\n") + 1 == 4:
        if len(subkey10_lines)==1:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data
        if len(subkey10_lines)==2:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data
        if len(subkey10_lines)==3:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data
        if len(subkey10_lines)==4:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data
        if len(subkey10_lines)==5:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data
        if len(subkey10_lines)==6:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data
        if len(subkey10_lines)==7:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data
        if len(subkey10_lines)==8:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data
        if len(subkey10_lines)==9:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data
        if len(subkey10_lines)==10:
            all_data = genre_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data + subkey10_10_data

    if genre.count("\n") + 1 == 8 or genre.count("\n") + 1 ==9:
        if len(subkey10_lines)==1:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data
        if len(subkey10_lines)==2:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data
        if len(subkey10_lines)==3:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data
        if len(subkey10_lines)==4:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data
        if len(subkey10_lines)==5:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data
        if len(subkey10_lines)==6:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data
        if len(subkey10_lines)==7:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data
        if len(subkey10_lines)==8:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data
        if len(subkey10_lines)==9:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data
        if len(subkey10_lines)==10:
            all_data = genre_1_top_result + genre_2_top_result + wiki_top_result + subkey10_1_data + subkey10_2_data + subkey10_3_data + subkey10_4_data + subkey10_5_data + subkey10_6_data + subkey10_7_data + subkey10_8_data + subkey10_9_data + subkey10_10_data



    # ファイルに書き込んで保存
    if os.path.exists("test.txt"):
        with open("test.txt", "a", encoding="utf-8") as f:

            # 現在の日付を取得
            current_date = datetime.datetime.now()
            text = f"{topic}\n始め\n{all_data}\n{topic}\n終わり"
            f.write(f"\n{current_date.strftime('%Y-%m-%d %H:%M:%S')},{text}")
    else:
        with open("test.txt", "w", encoding="utf-8") as f:
            current_date = datetime.datetime.now()
            text = f"{topic}\n始め\n{all_data}\n{topic}\n終わり"
            f.write(f"{current_date.strftime('%Y-%m-%d %H:%M:%S')},{text}")


    # トークン数で行ごとに分割
    import unicodedata

    text_all = all_data
    text = text_all.replace('\n', '')
    lines = text.replace("。", "。\n").replace(".",".\n").split('\n')

    # 空行を削除
    lines = [line for line in lines if line.strip() != ""]

    # 行数をカウント
    line_count = len(lines)
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    lines = "".join(lines)
    tokens_all = tokenizer.tokenize(text)

    if len(tokens_all) > 6800:
        line_lengths = [len(tokenizer.tokenize(line)) for line in lines]
        total = 0
        index_list = []
        for i, length in enumerate(line_lengths):
            total += length
            if total > 6800:
                index_list.append(i)
                total =  line_lengths[i]

    else:
        index_list = []


    if len(tokens_all) > 6800:

        # 6800トークン以下になるようにデータを行ごとに分割
        for i in range(len(index_list)):
            over_6800 = index_list[i]

            if i == 0:
              first_group_end = over_6800
              first_group_lines = lines[0 : first_group_end]
              first_group_text = ''.join(first_group_lines)
              if i == 0 and len(index_list) == 1:
                  end = line_count + 1
                  remain_group = lines[first_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 1:
              second_group_end = over_6800
              second_group_lines = lines[first_group_end : second_group_end]
              second_group_text = ''.join(second_group_lines)
              if i == 1 and len(index_list) == 2:
                  end = line_count + 1
                  remain_group = lines[second_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 2:
              third_group_end  = over_6800
              third_group_lines = lines[second_group_end : third_group_end]
              third_group_text = ''.join(third_group_lines)
              if i == 2 and len(index_list) == 3:
                  end = line_count + 1
                  remain_group = lines[third_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 3:
              fourth_group_end = over_6800
              fourth_group_lines = lines[third_group_end : fourth_group_end]
              fourth_group_text = ''.join(fourth_group_lines)
              if i == 3 and len(index_list) == 4:
                  end = line_count + 1
                  remain_group = lines[fourth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 4:
              fifth_group_end = over_6800
              fifth_group_lines = lines[fourth_group_end : fifth_group_end]
              fifth_group_text = ''.join(fifth_group_lines)
              if i == 4 and len(index_list) == 5:
                  end = line_count + 1
                  remain_group = lines[fifth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 5:
              sixth_group_end = over_6800
              sixth_group_lines = lines[fifth_group_end : sixth_group_end]
              sixth_group_text = ''.join(sixth_group_lines)
              if i == 5 and len(index_list) == 6:
                  end = line_count + 1
                  remain_group = lines[sixth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 6:
              seventh_group_end = over_6800
              seventh_group_lines = lines[sixth_group_end : seventh_group_end]
              seventh_group_text = ''.join(seventh_group_lines)
              if i == 6 and len(index_list) == 7:
                  end = line_count + 1
                  remain_group = lines[seventh_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 7:
              eighth_group_end = over_6800
              eighth_group_lines = lines[seventh_group_end : eighth_group_end]
              eighth_group_text = ''.join(eighth_group_lines)
              if i == 7 and len(index_list) == 8:
                  end = line_count + 1
                  remain_group = lines[eighth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 8:
              ninth_group_end = over_6800
              ninth_group_lines = lines[eighth_group_end : ninth_group_end]
              ninth_group_text = ''.join(ninth_group_lines)
              if i == 8 and len(index_list) == 9:
                  end = line_count + 1
                  remain_group = lines[ninth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 9:
              tenth_group_end = over_6800
              tenth_group_lines = lines[ninth_group_end : tenth_group_end]
              tenth_group_text = ''.join(tenth_group_lines)
              if i == 9 and len(index_list) == 10:
                  end = line_count + 1
                  remain_group = lines[tenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 10:
              eleventh_group_end = over_6800
              eleventh_group_lines = lines[tenth_group_end : eleventh_group_end]
              eleventh_group_text = ''.join(eleventh_group_lines)
              if i == 10 and len(index_list) == 11:
                  end = line_count + 1
                  remain_group = lines[eleventh_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 11:
              twelveth_group_end = over_6800
              twelveth_group_lines = lines[eleventh_group_end : twelveth_group_end]
              twelveth_group_text = ''.join(twelveth_group_lines)
              if i == 11 and len(index_list) == 12:
                  end = line_count + 1
                  remain_group = lines[twelveth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 12:
              thirteenth_group_end = over_6800
              thirteenth_group_lines = lines[twelveth_group_end : thirteenth_group_end]
              thirteenth_group_text = ''.join(thirteenth_group_lines)
              if i == 12 and len(index_list) == 13:
                  end = line_count + 1
                  remain_group = lines[thirteenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 13:
              fourteenth_group_end = over_6800
              fourteenth_group_lines = lines[thirteenth_group_end : fourteenth_group_end]
              fourteenth_group_text = ''.join(fourteenth_group_lines)
              if i == 13 and len(index_list) == 14:
                  end = line_count + 1
                  remain_group = lines[fourteenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 14:
              fifteenth_group_end = over_6800
              fifteenth_group_lines = lines[fourteenth_group_end : fifteenth_group_end]
              fifteenth_group_text = ''.join(fifteenth_group_lines)
              if i == 14 and len(index_list) == 15:
                  end = line_count + 1
                  remain_group = lines[fifteenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 15:
              sixteenth_group_end = over_6800
              sixteenth_group_lines = lines[fifteenth_group_end : sixteenth_group_end]
              sixteenth_group_text = ''.join(sixteenth_group_lines)
              if i == 15 and len(index_list) == 16:
                  end = line_count + 1
                  remain_group = lines[sixteenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 16:
              seventeenth_group_end = over_6800
              seventeenth_group_lines = lines[sixteenth_group_end : seventeenth_group_end]
              seventeenth_group_text = ''.join(seventeenth_group_lines)
              if i == 16 and len(index_list) == 17:
                  end = line_count + 1
                  remain_group = lines[seventeenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 17:
              eighteenth_group_end = over_6800
              eighteenth_group_lines = lines[seventeenth_group_end : eighteenth_group_end]
              eighteenth_group_text = ''.join(eighteenth_group_lines)
              if i == 17 and len(index_list) == 18:
                  end = line_count + 1
                  remain_group = lines[eighteenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 18:
              nineteenth_group_end = over_6800
              nineteenth_group_lines = lines[eighteenth_group_end : nineteenth_group_end]
              nineteenth_group_text = ''.join(nineteenth_group_lines)
              if i == 18 and len(index_list) == 19:
                  end = line_count + 1
                  remain_group = lines[nineteenth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 19:
              twentyth_group_end = over_6800
              twentyth_group_lines = lines[nineteenth_group_end : twentyth_group_end]
              twentyth_group_text = ''.join(twentyth_group_lines)
              if i == 19 and len(index_list) == 20:
                  end = line_count + 1
                  remain_group = lines[twentyth_group_end : end]
                  remain_group_text = ''.join(remain_group)

            if i == 20:
              twentyoneth_group_end = over_6800
              twentyoneth_group_lines = lines[twentyth_group_end : twentyoneth_group_end]
              twentyoneth_group_text = ''.join(twentyoneth_group_lines)
              if i == 20 and len(index_list) == 21:
                  end = line_count + 1
                  remain_group = lines[twentyoneth_group_end : end]
                  remain_group_text = ''.join(remain_group)


    # 使用するデータをランダム選択
    if len(index_list)==0:
        short_data = all_data

    elif len(index_list) == 1:
        first_data = first_group_text
        second_data = remain_group_text

    elif len(index_list) == 2:
        first_data = first_group_text
        second_data = second_group_text

    else:
        data_number = random.sample(range(len(index_list)),2)
        first_data_number = data_number[0]
        second_data_number = data_number[1]

        if first_data_number == 0:
            first_data = first_group_text
        if first_data_number == 1:
            first_data = second_group_text
        if first_data_number == 2:
            first_data = third_group_text
        if first_data_number == 3:
            first_data = fourth_group_text
        if first_data_number == 4:
            first_data = fifth_group_text
        if first_data_number == 5:
            first_data = sixth_group_text
        if first_data_number == 6:
            first_data = seventh_group_text
        if first_data_number == 7:
            first_data = eighth_group_text
        if first_data_number == 8:
            first_data = ninth_group_text
        if first_data_number == 9:
            first_data = tenth_group_text
        if first_data_number == 10:
            first_data = eleventh_group_text
        if first_data_number == 11:
            first_data = twelveth_group_text
        if first_data_number == 12:
            first_data = thirteenth_group_text
        if first_data_number == 13:
            first_data = fourteenth_group_text
        if first_data_number == 14:
            first_data = fifteenth_group_text
        if first_data_number == 15:
            first_data = sixteenth_group_text
        if first_data_number == 16:
            first_data = seventeenth_group_text
        if first_data_number == 17:
            first_data = eighteenth_group_text
        if first_data_number == 18:
            first_data = nineteenth_group_text
        if first_data_number == 19:
            first_data = twentyth_group_text
        if first_data_number == 20:
            first_data = twentyoneth_group_text

        if second_data_number == 0:
            second_data = first_group_text
        if second_data_number == 1:
            second_data = second_group_text
        if second_data_number == 2:
            second_data = third_group_text
        if second_data_number == 3:
            second_data = fourth_group_text
        if second_data_number == 4:
            second_data = fifth_group_text
        if second_data_number == 5:
            second_data = sixth_group_text
        if second_data_number == 6:
            second_data = seventh_group_text
        if second_data_number == 7:
            second_data = eighth_group_text
        if second_data_number == 8:
            second_data = ninth_group_text
        if second_data_number == 9:
            second_data = tenth_group_text
        if second_data_number == 10:
            second_data = eleventh_group_text
        if second_data_number == 11:
            second_data = twelveth_group_text
        if second_data_number == 12:
            second_data = thirteenth_group_text
        if second_data_number == 13:
            second_data = fourteenth_group_text
        if second_data_number == 14:
            second_data = fifteenth_group_text
        if second_data_number == 15:
            second_data = sixteenth_group_text
        if second_data_number == 16:
            second_data = seventeenth_group_text
        if second_data_number == 17:
            second_data = eighteenth_group_text
        if second_data_number == 18:
            second_data = nineteenth_group_text
        if second_data_number == 19:
            second_data = twentyth_group_text
        if second_data_number == 20:
            second_data = twentyoneth_group_text

    print(first_data)
    print("------------------------------------")
    print(second_data)


    # クイズ生成
    from langchain.chains import ConversationChain
    from langchain import PromptTemplate

    template = f"""あなたは以下の3つの問題形式で、インプットされた情報のみを使って{topic}に関する難しい問題を作成してください。単純ではなく、回答者が考えさせられるような正確な内容の良問を作成してください。""" + """必ず各問題形式について1つずつ、合計3つの問題を作成してください。必ず質問文は「Q:」から始め、答えは「A:」から始めてください。必ず解説は表示しないでください。
                1.選択肢付き問題: 質問文に続いて、選択肢A, B, C, Dの形式で4つの選択肢を含めてください。正解の選択肢も明示してください。
                例:
                Q: 問題文
                (A) 選択肢A
                (B) 選択肢B
                (C) 選択肢C
                (D) 選択肢D
                A: 正解の選択肢
                2.穴埋め問題: 質問文の中に'___'（アンダーバー3つ）を使って、解答者が埋めるべき単語やフレーズの場所を示してください。答えは改行して表示してください。
                3.並び替え問題: 質問文の中に並び替えるべき情報を示し、解答者にそれらを正しい順序に並べ替えるよう指示してください。質問文の並び替えるべき時系列情報には番号をつけてください。答えは改行せずに表示してください。必ず解説は表示しないでください。
                インプットされた情報:{input} """

    # プロンプトテンプレート
    prompt_template = PromptTemplate(
                            input_variables   = ["input"],
                            template          = template,
                            validate_template = True,
                            )


    from langchain.llms import OpenAI
    LLM = OpenAI(
                model_name = "gpt-4",
                temperature = 1.2,
                max_tokens = 1000,
                top_p = 0,
                frequency_penalty = 0.5,
                presence_penalty  = 0,
                n = 1,
                max_retries = 6,
                )


    chain = LLMChain(
                    llm     = LLM,
                    prompt  = prompt_template,
                    verbose = True,
                    )


    if len(index_list)==0:
        quiz_text = chain.predict(input = short_data)
        print(quiz_text)

    else:
        quiz_text_1 = chain.predict(input = first_data)
        quiz_text_2 = chain.predict(input = second_data)
        print(quiz_text_1)
        print(quiz_text_2)
